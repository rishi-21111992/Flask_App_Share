# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WilaCTZxfj4wTvi32PJzEqeJ2u-0t_f_

This Project is to build a model that will improve the recommendations given to the users given their past reviews and ratings.

In order to do this, it is conformed from the previous steps that Linear Regression Moldel and User Based Recommendation System gives the highest accuracy among the other models. 

Hence, first Linear Regression model is built and proceeding further with User Based Recommendation System.

Data sourcing and sentiment analysis by Linear Regression Model

Import and Install useful packages.
"""

## General Purpose Library

import re
import time
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
from datetime import datetime
import warnings
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score,balanced_accuracy_score

warnings.filterwarnings("ignore") 
pd.set_option('display.max_columns', 200)
pd.set_option('display.max_colwidth', 300)

# NLTK libraries
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk import FreqDist
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import wordnet

#Modelling 
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, classification_report

product_data = pd.read_csv("/content/sample30.csv")

"""**EDA of the data set given**"""

## null column check
product_data.isnull().sum()

product_data.info()

## dropping null values from user_sentiment column
df = product_data.dropna(subset=['user_sentiment'])

## checking null values columns

df.isnull().sum()

#plotting application data distribution
plt.figure(figsize= (10,10))
sns.countplot(x='user_sentiment', data= df)
plt.show()

## total no. in each user_sentiment
product_data['user_sentiment'].value_counts()

# mapping the positive sentiments as 1 and negative as 0
df['user_sentiment']= df['user_sentiment'].map({'Positive':1 , 'Negative':0})

"""**Text Preprocessing**"""

#converting into string
df['reviews_text'] = df['reviews_text'].astype('str')

# Remove punctuation 
df['reviews_text'] = df['reviews_text'].str.replace('[^\w\s]','')

# Remove Stopwords
stop = stopwords.words('english')
df['reviews_text'] = df['reviews_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

def scrub_words(text):
    """Basic cleaning of texts."""
    
    # remove html markup
    text=re.sub("(<.*?>)"," ",text)
    
    # remove unneccessary words
    text = text.replace("RE:","")
    text = text.replace("FW:","")
    text = text.replace("_"," ")
    
    #remove non-ascii and digits
    text=re.sub("(\\W|\\d)"," ",text)
    
    #remove whitespace
    text = text.strip()
    text = re.sub(' +', ' ',text)
    
    return text

## applying scrub_words fuction through lambda
df['reviews_text']=df['reviews_text'].apply(lambda x: scrub_words(x))

#loercase the reviews_text
df['reviews_text']= df['reviews_text'].str.lower()

## creating lemmatizer object 
lemmatizer = nltk.stem.WordNetLemmatizer()

def nltk_tag_to_wordnet_tag(nltk_tag):
    if nltk_tag.startswith('J'):
        return wordnet.ADJ
    elif nltk_tag.startswith('V'):
        return wordnet.VERB
    elif nltk_tag.startswith('N'):
        return wordnet.NOUN
    elif nltk_tag.startswith('R'):
        return wordnet.ADV
    else:
        return None

def lemmatize_sentence(sentence):
    #tokenize the sentence and find the POS tag for each token
    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))
    #tuple of (token, wordnet_tag)
    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)
    lemmatized_sentence = []
    for word, tag in wordnet_tagged:
        if tag is None:
            #if there is no available tag, append the token as is
            lemmatized_sentence.append(word)
        else:
            #else use the tag to lemmatize the token
            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))
    return " ".join(lemmatized_sentence)

# assigning x= reviews & y=sentiment

x=df['reviews_text'] 
y=df['user_sentiment']

no_of_classes= len(pd.Series(y).value_counts())
print(no_of_classes)

"""Checking the distribution of class for class imbalance"""

#Distribution of the target variable data in terms of proportions.
for i in range(0,no_of_classes):
  print("Percent of {0}s: ".format(i), round(100*pd.Series(y).value_counts()[i]/pd.Series(y).value_counts().sum(),2), "%")

from xgboost import XGBClassifier
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import pickle
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.feature_extraction.text import CountVectorizer

X_train, X_test, y_train, y_test = train_test_split(df['reviews_text'], 
                                                        df['user_sentiment'], test_size=0.2, random_state=42)

"""Creating a generic function which do below action
1. convert into vector the input 
2. as there are class imbalance present in the dataset handle that
3. save the vector pickle file
4. train the model 
5. save the model pickle file 
6. predict the test data
7. show the matrics of model performance

"""

def model_split(df) :
    '''This function splits data to train and test, then vectorized reviews '''
    
    # split train-test
    X_train, X_test, y_train, y_test = train_test_split(df['reviews_text'], 
                                                        df['user_sentiment'], test_size=0.2, random_state=42)
    print("X train ",X_train.shape)
    print("X test " ,X_test.shape)
        
    # define vectorize and fit to data     
    word_vectorizer = TfidfVectorizer(sublinear_tf=True,strip_accents='unicode',
        analyzer='word',token_pattern=r'\w{1,}',stop_words='english')

    word_vectorizer.fit(df['reviews_text'])
    
    # train - test vectorized features - tranforming to suitable format for modeling
    train_word_features = word_vectorizer.transform(X_train) 
    test_word_features = word_vectorizer.transform(X_test)

    # handling class imbalance 
    counter = Counter(y_train)
    print('Before',counter)
    sm = SMOTE()
    X_train_transformed_sm, y_train_sm = sm.fit_resample(train_word_features, y_train)
    counter = Counter(y_train_sm)
    print('After',counter)
    pickle.dump(word_vectorizer.vocabulary_, open("vector.pkl", "wb"))
    return X_train_transformed_sm , test_word_features, y_train_sm, y_test 

def model(df,classifier,filename):
    '''this function gives modeling results and confusion matrix also'''
    train_word_features,test_word_features,y_train,y_test = model_split(df)
    classifier.fit(train_word_features, y_train)
    
    # calculating results 
    y_pred_train = classifier.predict(train_word_features)
    y_pred = classifier.predict(test_word_features)
    
    #for smart printing (learned from our lead instructor Bryan Arnold)
    print("Accuracy:"); print("="*len("Accuracy:"))
    print(f"TRAIN: {accuracy_score(y_train, y_pred_train)}")
    print(f"TEST: {accuracy_score(y_test, y_pred)}")

    print("\nBalanced Accuracy:"); print("="*len("Balanced Accuracy:"))
    print(f"TRAIN: {balanced_accuracy_score(y_train, y_pred_train)}")
    print(f"TEST: {balanced_accuracy_score(y_test, y_pred)}")
    print(y_pred.shape)
    print(classification_report(y_pred_train, y_train))
    with open(filename, 'wb') as files:
      pickle.dump(classifier, open(filename, 'wb'))
    # plot confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plot_confusion_matrix(cm)
    plt.show()

from mlxtend.plotting import plot_confusion_matrix

### checking through LogisticRegression

classifier = LogisticRegression(class_weight = "balanced", C=0.5, solver='sag')
model(df,classifier,"LRModel")

test = df[df['name'] == 'Red (special Edition) (dvdvideo)']
test_data = test['reviews_text']
test_data

"""Building a recommendation system

*   User based recommendation
*   User based prediction & evaluation

"""

# import libraties
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#reading ratings file
reviews = pd.read_csv('/content/sample30.csv')

reviews.head()

"""**Dividing train and test dataset**"""

# Test and Train split of the dataset.
from sklearn.model_selection import train_test_split
train, test = train_test_split(reviews, test_size=0.30, random_state=31)

print(train.shape)
print(test.shape)

# Pivot the train ratings dataset into matrix format in which columns are product_names and the rows are reviewers_name.
df_pivot= train.pivot_table(
    index='reviews_username',
    columns='name',
    values='reviews_rating'
#    aggfunc=lambda x: ' '.join(x.dropna())
).fillna(0)

df_pivot.head()

## checking sample username
reviews[reviews['reviews_username'] =='joshua']

"""### Creating dummy train & dummy test dataset
These dataset will be used for prediction 
- Dummy train will be used later for prediction of the products which has not been rated by the user. To ignore the products rated by the user, we will mark it as 0 during prediction. The products not rated by user is marked as 1 for prediction in dummy train dataset. 

- Dummy test will be used for evaluation. To evaluate, we will only make prediction on the products rated by the user. So, this is marked as 1. This is just opposite of dummy_train.
"""

# Copy the train dataset into dummy_train

dummy_train = train.copy()

# The products not rated by user is marked as 1 for prediction. 
dummy_train['reviews_rating'] = dummy_train['reviews_rating'].apply(lambda x: 0 if x>=1 else 1)

dummy_train.head()

# Convert the dummy train dataset into matrix format.
dummy_train= train.pivot_table(
    index='reviews_username',
    columns='name',
    values='reviews_rating'
).fillna(1)

dummy_train.head()

"""**Cosine Similarity**

Cosine Similarity is a measurement that quantifies the similarity between two vectors [Which is Rating Vector in this case] 

**Adjusted Cosine**

Adjusted cosine similarity is a modified version of vector-based similarity where we incorporate the fact that different users have different ratings schemes. In other words, some users might rate items highly in general, and others might give items lower ratings as a preference. To handle this nature from rating given by user , we subtract average ratings for each user from each user's rating for different movies.

# User Similarity Matrix

## Using Cosine Similarity
"""

# Creating the User Similarity Matrix using pairwise_distance function.

from sklearn.metrics.pairwise import pairwise_distances

user_correlation = 1 - pairwise_distances(df_pivot, metric='cosine')
user_correlation[np.isnan(user_correlation)] = 0

print(user_correlation)

user_correlation.shape

"""## Using adjusted Cosine 

### Here, we are not removing the NaN values and calculating the mean only for the movies rated by the user

"""

# Create a user-movie matrix.
df_pivot = train.pivot_table(
    index='reviews_username',
    columns='name',
    values='reviews_rating'
)

"""### Normalising the rating of the product for each user around 0 mean"""

mean = np.nanmean(df_pivot, axis=1)
df_subtracted = (df_pivot.T-mean).T

df_subtracted.head()

"""### Finding cosine similarity"""

# Creating the User Similarity Matrix using pairwise_distance function.
user_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')
user_correlation[np.isnan(user_correlation)] = 0
print(user_correlation)

"""## Prediction - User User

Doing the prediction for the users which are positively related with other users, and not the users which are negatively related as we are interested in the users which are more similar to the current users. So, ignoring the correlation for values less than 0. 
"""

user_correlation[user_correlation<0]=0
user_correlation

"""Rating predicted by the user (for products rated as well as not rated) is the weighted sum of correlation with the product rating (as present in the rating dataset). """

user_predicted_ratings = np.dot(user_correlation, df_pivot.fillna(0))
user_predicted_ratings

"""Since we are interested only in the products not rated by the user, we will ignore the products rated by the user by making it zero. """

user_final_rating = np.multiply(user_predicted_ratings,dummy_train)
user_final_rating.head()

"""# Evaluation - User User 

Evaluation will we same as you have seen above for the prediction. The only difference being, you will evaluate for the product already rated by the user instead of predicting it for the product not rated by the user. 
"""

# Find out the common users of test and train dataset.
common = test[test.reviews_username.isin(train.reviews_username)]
common.shape

common.head()

# convert into the user-movie matrix.
common_user_based_matrix = common.pivot_table(index='reviews_username', columns='name', values='reviews_rating')

# Convert the user_correlation matrix into dataframe.

user_correlation_df = pd.DataFrame(user_correlation)

user_correlation_df['reviews_username'] = df_subtracted.index
user_correlation_df.set_index('reviews_username',inplace=True)
user_correlation_df.head()

list_name = common.reviews_username.tolist()

user_correlation_df.columns = df_subtracted.index.tolist()


user_correlation_df_1 =  user_correlation_df[user_correlation_df.index.isin(list_name)]

user_correlation_df_2 = user_correlation_df_1.T[user_correlation_df_1.T.index.isin(list_name)]
user_correlation_df_2

user_correlation_df_3 = user_correlation_df_2.T
user_correlation_df_3

user_correlation_df_3.shape

user_correlation_df_3[user_correlation_df_3<0]=0

common_user_predicted_ratings = np.dot(user_correlation_df_3, common_user_based_matrix.fillna(0))
common_user_predicted_ratings

dummy_test = common.copy()

dummy_test['reviews_rating'] = dummy_test['reviews_rating'].apply(lambda x: 1 if x>=1 else 0)

dummy_test = dummy_test.pivot_table(index='reviews_username', columns='name', values='reviews_rating').fillna(0)

dummy_test.shape

common_user_predicted_ratings = np.multiply(common_user_predicted_ratings,dummy_test)
common_user_predicted_ratings.head(2)

"""Calculating the RMSE for only the products rated by user. For RMSE, normalising the rating to (1,5) range."""

from sklearn.preprocessing import MinMaxScaler
from numpy import *

X  = common_user_predicted_ratings.copy() 
X = X[X>0]

scaler = MinMaxScaler(feature_range=(1, 5))
print(scaler.fit(X))
y = (scaler.transform(X))

print(y)

common_ = common.pivot_table(index='reviews_username', columns='name', values='reviews_rating')

# Finding total non-NaN value
total_non_nan = np.count_nonzero(~np.isnan(y))

rmse = (sum(sum((common_ - y )**2))/total_non_nan)**0.5
print("RMSE of User based Similarity : ",rmse)

import pickle 
pickle.dump(user_final_rating, open("user_based_recomm.pkl", "wb"))

def prediction(name):
    reviews = pd.read_csv('/content/sample30.csv')
    pipeline = pickle.load(open('user_based_recomm.pkl', 'rb'))
    sr = pipeline.loc[name].sort_values(ascending=False)[0:20] ## series
    top_20_products = pd.DataFrame({'name':sr.index})
    top_20_reviews = reviews[reviews['name'].isin(top_20_products['name'])][['name','reviews_text']] 
    transformer = TfidfTransformer()
    loaded_vec = CountVectorizer(decode_error="replace",vocabulary=pickle.load(open('vector.pkl', 'rb')))
    test_data_features = transformer.fit_transform(loaded_vec.fit_transform(top_20_reviews['reviews_text']))
    loaded_model = pickle.load(open("LRModel", 'rb'))
    result1 = loaded_model.predict(test_data_features)
    top_20_reviews['sentiment'] = result1.tolist()
    top = top_20_reviews.groupby(['name']).mean()
    top5 = top.sort_values(by='sentiment',ascending=False)[:5]
    top5.reset_index(level=0, inplace=True)
    top_5_products= top5['name']
    top_5_product = pd.DataFrame({'name':top_5_products})
    return top_5_product